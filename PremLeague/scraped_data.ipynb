{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "\n",
    "data = requests.get(standings_url)\n",
    "\n",
    "time.sleep(5) # adding a delay to avoid overloading the server\n",
    "\n",
    "data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data.text)\n",
    "\n",
    "standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "if not standings_table:\n",
    "    print(\"No standings table found.\")\n",
    "else:\n",
    "    standings_table = soup.select('table.stats_table')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses BeautifulSoup to parse the HTML content stored in `data.text`. It then attempts to select the first table with the class `stats_table`. If no standings table is found, it prints a message; otherwise, it assigns the selected table to the variable `standings_table`. (The if statement is added because I kept running into errors because the table index was out of bounds due to me sending too many requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = standings_table.find_all('a')\n",
    "links = [l.get('href') for l in links]\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "\n",
    "links\n",
    "\n",
    "team_urls = [f\"https://fbref.com{l}\" for l in links] ## formatting back to links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code retrieves all anchor tags from the `standings_table` to find hyperlinks. It then extracts the `href` attributes from these tags to create a list of URLs. The list is filtered to keep only those links that contain `'/squads/'`, which are team-specific pages. Finally, it constructs full URLs for each team by adding the base URL `https://fbref.com` to the filtered links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    data = requests.get(standings_url)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "    links = [l for l in links if '/squads/' in l]\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
    "    standings_url = f\"https://fbref.com{previous_season}\"\n",
    "    \n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        data = requests.get(team_url)\n",
    "        matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "        soup = BeautifulSoup(data.text)\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n",
    "        shooting.columns = shooting.columns.droplevel()\n",
    "        try:\n",
    "            team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "        team_data = team_data[team_data[\"Comp\"] == \"Premier League\"]\n",
    "        \n",
    "        team_data[\"Season\"] = year\n",
    "        team_data[\"Team\"] = team_name\n",
    "        all_matches.append(team_data)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code iterates over a list of years to scrape Premier League match data for each team. It fetches the standings data and extracts team URLs from the standings table. The code then updates the standings URL to point to the previous season's data. For each team, it retrieves match and shooting statistics, merges the datasets, and filters for Premier League matches. Then it appends the processed data to a list while adding a delay between requests so it doesn't overload the server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df = pd.concat(all_teams) \n",
    "stat_df.to_csv(\"matches.csv\") ## importing to csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
